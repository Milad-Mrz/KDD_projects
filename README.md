1-Presentation and make files <br/>
2-answering the questions after the presentation <br/>

A- There are several data sources available. Decide by yourself, which data you want to concern in your project (Question A). done <br/>

B- Download the data; decide and justify how you manage the data in your management system.(Question B). done <br/>

C- Prepare the data, for instance by inspecting + discussing each attribute by a visualisation, by repairing the data if needed, by adding additional 
attributes if beneficial, et cetera. Use the terminolgoy and justify your decision (Question C). <br/>

D- Analyse your data with an application of your choice: Association Discovery, Clustering, Classification, or another application (Question D).
E and F- Interpret and discuss the results (Question E) and proposed a resulting activity (Question F) <br/>


3- Summary of up to 1500 words (without bibliography; screenshots) <br/>
Please let me know who is responsible for which task <br/>



---
Ok, I have checked the first one, it's a good start and gives lots of insight. Some ranges need to be fixed, for example wind speed, which does not show all the data, and i don't know why it doesn't include "day.csv" ?

for "KDD_1Preprocessing.py" We also need some plots of all dependent attributes so we can show in our presentation if some attributes are not needed and justify the reduction of some dimensions. for instance, 'temp' and 'atemp'

in "KDD_2Modeling.py," I have included all models and their evaluations
because they are very related codes, but you can add visualizations of evaluations in "KDD_5Evaluation.py" file. You can call the "modelSelector" function from "KDD_2Modeling.py" and use the outputs to plot all models vs evaluation parameters like
ROC Curve and AUC: 'auc'
False Positive Rate: 'fpr', True Positive Rate: 'tpr'
Accuracy: 'acc', Precision: 'prec', Recall: 'rec', F1-Score: 'f1', Kappa Score: 'kappa', Confusion Matrix: 'cm'

---

beside those plots i mentioned to Martina, we need these steps for "KDD_1Preprocessing.py":

1-Handling duplicate data
2-Handling invalid data (e.g., negative age values or dates that are in the future)
3-Handling outliers and extreme values
4-Handling missing data (e.g., imputation or removal)
5-Handling data normalization or scaling
6-Removing unnecessary columns or features, reducing dimension, or reducing dependent attributes
7-Addressing imbalance in the target attribute
[18:08, 11/05/2023] Milad: And this is just for the code, and after that we have these things to do:

- writing 1500 words report for 
- create a power point 
- get ready for presentation

so I think we should finish the code by this monday (15th) and then spend the rest on report and ...






## A Knowledge Discovery and Data Mining Analysis of Hourly Bike-Sharing Patterns

<p align="center">
  <img src="./images/3.png" alt="Social_Biking" height="300">
</p>

1- **Introduction:** 

This project aims to analyze the patterns and trends in hourly bike-sharing usage. With over 500 bike-sharing programs worldwide and over 500,000 bicycles, bike-sharing systems have become a popular and important mode of transportation, contributing significantly to traffic, environmental, and health-related issues.

The data generated by these systems provides a wealth of information about the travel patterns, user behavior, and preferences, making them a valuable resource for research. Unlike other transport services, bike-sharing systems explicitly record the duration of travel, departure and arrival positions, providing a virtual sensor network that can be used to sense mobility in the city. Through SocialBikeMine, we aim to use advanced data mining techniques to uncover meaningful insights into the usage patterns of bike-sharing systems, and their impact on the city's mobility and sustainability.

- a brief overview of the project
- the purpose of the analysis, and the dataset being used
- It may also include any relevant background information and research questions.

2- **Data Description:** 

This section provides a detailed description of the data being analyzed, including the data source, data size, variables and their types, and any preprocessing steps performed.

3- **Data Exploration:** 

This section presents an overview of the data and explores the distribution, relationships, and patterns of the variables. It may include summary statistics, visualizations, and exploratory data analysis techniques.

4- **Data Preparation:** 

This section describes the data preparation steps performed to transform and clean the data for analysis. It may include data cleaning, feature engineering, normalization, and other preprocessing techniques.

5- **Modeling:** 

This section describes the modeling techniques used to analyze the data and make predictions or classifications. It may include a description of the algorithms used, parameter tuning, model selection, and validation techniques.

6- **Results:** 

This section presents the results of the analysis, including any insights or conclusions derived from the data. It may include visualizations, summary statistics, and other relevant metrics.

7- **Discussion:** 

This section discusses the implications of the results, their significance, and any limitations or challenges faced during the analysis. It may also provide recommendations for future research or improvements to the analysis.

8- **Conclusion:** 

This section summarizes the findings of the analysis and their implications, and may provide a brief summary of the work done and its significance.

9- **References:** 

This section lists any sources referenced or cited throughout the analysis, including datasets, research papers, and other relevant resources.


